# ğŸ“š AIRCode

This is the official implementation of the paper AIRCode: Automatic Iterative Prompt Refinement for Code
Generation

## ğŸš€ Quick Start

### âš™ï¸ Preparation

1. **Environmental** settings: `pip install -r requirements.txt`
2. **Data** Preparation: The dataset used in Code Generation are in : `./code_generation/Dataset`, The CodeNet and AVATAR dataset used in Code Translation can be found  [here](https://gofile.io/d/HFv3VX). 
3. **OpenAI API key** required: add your OpenAI API key, base url and other related settings in the `.env` file. You can also customize these two parameters to use the DeepSeek Model.



### ğŸ’» Code Generation

#### 0. Automatically Generate Training Set

â€‹    Use the following instructions to automatically generate training dataset using a large language model, as described in Section 2.2 of the paper.  `--output_path` parameter is used to specify the file output path.

```python
python 0_train_set_generate.py --output_path ./train_set_gpt3.5turbo.jsonl
```

â€‹    Run the `train_set_postprocessing.py` script in the `code_generation/utils` directory to post-process the generated results.

```python
python train_set_postprocessing --file_path=../train_set_gpt3.5turbo.jsonl
```

â€‹    The format of the processed training dataset may appear as follows:

```json
{"task_id": "auto/0", "prompt": "from typing AIRCodert List\n\n\ndef custom_merge_lists(list1: List[int], list2: List[int]) -> List[int]:\n    ", "entry_point": "custom_merge_lists", "canonical_solution": "    merged_list = []\n    len1, len2 = len(list1), len(list2)\n    max_len = max(len1, len2)\n\n    for i in range(max_len):\n        if i < len1:\n            merged_list.append(list1[i])\n        if i < len2:\n            merged_list.append(list2[i])\n\n    return merged_list\n", "test": "\n\nMETADATA = {\n    'author': 'assistant',\n    'dataset': 'generated'\n}\n\n\ndef check(candidate):\n    assert candidate([1, 2, 3, 4], [5, 6, 7]) == [1, 5, 2, 6, 3, 7, 4]\n    assert candidate([1, 2, 3], [4, 5, 6, 7]) == [1, 4, 2, 5, 3, 6, 7]\n    assert candidate([1, 2], [4, 5, 6, 7]) == [1, 4, 2, 5, 6, 7]\n"}
{"task_id": "auto/1", "prompt": "from typing AIRCodert List\n\n\ndef find_max_subarray_sum(nums: List[int]) -> int:\n    ", "entry_point": "find_max_subarray_sum", "canonical_solution": "    max_sum = current_sum = nums[0]\n\n    for num in nums[1:]:\n        current_sum = max(num, current_sum + num)\n        max_sum = max(max_sum, current_sum)\n\n    return max_sum\n", "test": "\n\nMETADATA = {\n    'author': 'assistant',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert candidate([-2, 1, -3, 4, -1, 2, 1, -5, 4]) == 6\n    assert candidate([1, 2, 3, -2, 5]) == 9\n    assert candidate([-2, -3, 4, -1, -2, 1, 5, -3]) == 7\n    assert candidate([1, -2, 3, -1, 2, -3, 4]) == 6\n    assert candidate([1, -2, -3, 1, 3, -1, 2, -3, 4]) == 5\n    assert candidate([-1, -2, -3, -4, -5]) == -1\n\n"}
```

â€‹    

â€‹    The following instructions are used to randomly extract data from existing datasets:

```
python 0_train_set_select.py --input /path/to/input.jsonl --output /path/to/output.jsonl --sample_size 10
```

â€‹    The `--input` parameter specifies the input file path, the `--output` parameter specifies the output file path, and the `--sample_size` parameter represents the number of data samples to be extracted.



#### 1. Prompt Mutation.

â€‹    Use the following instructions to mutate the prompt. As described in Section 2.3 of the paper.

```
python 1_prompt_mutate.py --model gpt-3.5-turbo --prompt_path ./origin_prompt.jsonl --output_path ./data/mutated_prompt/gpt3.5turbo_epoch0.jsonl
```

â€‹    The `--model` parameter specifies the type of model to be used. If you need to use a DeepSeek series model, simply modify the `base_url` and `API_key` in `.env` to the properties provided by DeepSeek. After that, you can still use `--model` to specify the model version. The `prompt_path` attribute represents the file path where the original prompts to be mutated are stored, while `output_path` specifies the location where the mutated prompts will be saved.

â€‹    The format of the mutated prompt is as follows:

```json
{"prompt_id": 3, "mutated_prompt": "You are a code generation assistant tasked with creating a Python program based on natural language descriptions. The goal of the program is to successfully complete the tasks described in the natural language instructions and pass any test cases specific to those tasks.\n\nYour role is to generate Python code that accurately implements the desired functionality. In order to do this, you should carefully analyze the given task description and design a program that meets all the requirements.\n\nTo optimize the task's effectiveness, please ensure that your generated Python code includes the following:\n\n1. Detailed Task Description: Clearly define the task that needs to be accomplished. This ensures that the code you generate accurately addresses the desired functionality.\n\n2. Inputs and Outputs: Specify the inputs required for the program and the expected outputs. This helps in designing a solution that fulfills the expected requirements.\n\n3. Test Cases: Include specific test cases that the program should pass. These cases will be used to validate the correctness of the generated code.\n\n4. Code Structure and Guidance: Provide an outline or pseudo-code of the program's structure to guide the code generation process. This helps in organizing the code and ensures that it is easy to understand and follow.\n\nBy incorporating these elements into your generated Python code, you will create a more helpful and harmless response that accurately meets the requirements mentioned in the given task description."}
```



#### 2. Prompt Evaluation.

â€‹    Evaluate the obtained mutated prompts and generate the test set code execution results corresponding to each prompt.

```
python 2_prompt_evaluate.py --model gpt-3.5-turbo --trainset_path ./train_set_gpt3.5turbo.jsonl --mutated_prompt_path ./data/mutated_prompt/gpt3.5turbo_epoch0.jsonl --output_path ./data/prompt_evaluate/gpt3.5turbo_test
```

â€‹    The `--trainset_path` parameter is used to specify the path of the training dataset file, the `--mutated_prompt` attribute specifies the path of the file containing the set of prompts to be evaluated, and `--output_path` is used to designate the path where the generated code execution results will be saved.



#### 3. Prompt Update.

â€‹    Evaluate the performance of each prompt, select the best-performing prompts from this round of generated data, and save them as reference data for the next round of prompt mutation.

```
python 3_reinforcement_cal_score_and_select.py --evaluate_path ./data/prompt_evaluate/gpt3.5turbo_test --testset_path ./train_set_gpt3.5turbo.jsonl --origin_prompt ./data/mutated_prompt/gpt3.5turbo_epoch0.jsonl --best_prompt ./data/mutated_prompt/best_prompt_gpt3.5turbo_epoch0.jsonl
```

â€‹    The `--evaluate_path` parameter represents the path where the code files corresponding to the prompts from the previous round are saved. The `--origin_prompt` and `--best_prompt` parameters represent the original set of prompts and the updated best prompts.

â€‹    An example result is as follows:

```
Best prompts saved to ./data/mutated_prompt/best_prompt_gpt3.5turbo_epoch0.jsonl with prompt_ids: [8] and max weighted score: 56.68333333333333
```

â€‹    This represents the prompt that performed best on the test set in this round and will serve as the reference prompt for the next mutation. The corresponding code from step 1 can be used to directly utilize the resulting file for the next round of mutations.



â€‹    You can also modify the corresponding attributes in the `2+3+4_reinforcement.py` file based on the attribute values mentioned above, to complete the iterative steps 1-3 in one process. This continues until the convergence condition is met, i.e., the peak performance of the prompts on the training set remains unchanged in two consecutive prompt evaluation processes.



#### 4. Evaluation

â€‹    Modify the file path of the test set, prompt, and output file path attributes in the `prompt_test_humaneval_ET.py` file to test the performance of different prompts on the HumanEval, MBPP datasets, and their enhanced versions. The processed datasets are stored in the `./Dataset` directory.

â€‹    Use the `./post_processing.py` file to post-process the results returned by the model in order to extract the longest compilable Python code from the responses. Alternatively, you can use `./post_processing_modified.py` to directly extract Python code blocks wrapped in `python`. Then, use the following command:

```
evaluate_functional_correctness ./path_to_sanitized_result_file --problem_file=./corresponding_testset
```

â€‹    to evaluate the pass@1 value for the generated code result files.



### ğŸ’» Code Translation

â€‹    The process of mutating and updating prompts is similar to the one in the `code_generation` module. You only need to modify the corresponding attributes in the `Java2Python` and `Python2Java` folders. The meaning of each attribute is explained in the code files.

â€‹    After obtaining the `best_prompt` file, modify the `task_describe` attribute in the `evaluate_prompt_java2python.py` file to the value of the obtained `best_prompt`. This will generate the code results corresponding to the prompt.

â€‹    Once you modify the file paths in `run_python_testcases_avatar.py` accordingly, use the following command:

```
python run_python_testcases_avatar.py
```

â€‹    This will generate the pass@1 value for the code translation task, as well as the ratio of passed test cases to the total number of test cases. The same applies to the python2java  translation using   `evaluate_prompt_python2java.py` and `run_java_testcases_avatar.py` .



### ğŸ“Œ Notes

â€‹    It is worth noting that the models used for mutation and evaluation do not necessarily have to be the same. Considering time and cost factors, you can assign different models for these two processes.

#### ğŸ’¡Tips for Usage

â€‹    The size of the generated training set and the number of prompts produced in each iteration can have a certain impact on the performance of AIRCode. There exists a trade-off between cost and performance. In the paper, the training set includes 10 data points from each of three sources, and 10 prompts are generated per iteration. You can adjust these parameters as needed.

## ğŸ“ Framework

- For the pipeline of **AIRCode**, there are three main steps, with slight differences in the algorithms used for each process:

  - **Initialization**: Prompts are either manually written or generated by a large language model (e.g., ChatGPT) to create the initial training set. These prompts are processed and stored for mutation in the next stage.
  - **Mutation and Evaluation**: Prompts undergo mutation to create variations, which are then evaluated using test cases. The mutation process can use different models or approaches. Prompts are assessed for performance, and results are collected for each iteration.
  - **Update**: After each iteration, the best-performing prompts are selected for the next round of mutation using a weighted score mechanism. 

  This cycle continues until the performance of the prompts stabilizes or meets the desired criteria.

## ğŸŒ³ Code Structure

```python
.
â”œâ”€â”€ code_generation
â”‚   â”œâ”€â”€ 0_train_set_generate.py
â”‚   â”œâ”€â”€ 0_train_set_select.py
â”‚   â”œâ”€â”€ 1_prompt_mutate.py
â”‚   â”œâ”€â”€ 2+3+4_reinfocement.py
â”‚   â”œâ”€â”€ 2_prompt_evaluate.py
â”‚   â”œâ”€â”€ 3_cal_pass1_score_and_select_best_prompt.py
â”‚   â”œâ”€â”€ 3_reinforcement_cal_score_and_select.py
â”‚   â”œâ”€â”€ code_generation_testset.jsonl
â”‚   â”œâ”€â”€ data
â”‚   â”œâ”€â”€ Dataset
â”‚   â”‚   â”œâ”€â”€ HumanEval_ET.jsonl
â”‚   â”‚   â”œâ”€â”€ HumanEval_ET-processed-format.jsonl
â”‚   â”‚   â”œâ”€â”€ human-eval-v2-20210705.jsonl
â”‚   â”‚   â”œâ”€â”€ MBPP_ET-Formatted.jsonl
â”‚   â”‚   â”œâ”€â”€ MBPP_ET.jsonl
â”‚   â”‚   â”œâ”€â”€ mbpp.jsonl
â”‚   â”‚   â”œâ”€â”€ partial_updated_mbpp_format.jsonl
â”‚   â”‚   â””â”€â”€ updated_mbpp_format.jsonl
â”‚   â”œâ”€â”€ evalplus
â”‚   â”œâ”€â”€ human_eval
â”‚   â”œâ”€â”€ merge_result.py
â”‚   â”œâ”€â”€ origin_prompt.jsonl
â”‚   â”œâ”€â”€ post_processing_modified.py
â”‚   â”œâ”€â”€ post_processing.py
â”‚   â”œâ”€â”€ prompt_test_humaneval_ET.py
â”‚   â”œâ”€â”€ res
â”‚   â”‚   â”œâ”€â”€ ablation
â”‚   â”‚   â”‚   â”œâ”€â”€ humaneval_gpt3.5turbo_iter10.jsonl
â”‚   â”‚   â”‚   â””â”€â”€ humaneval_gpt3.5turbo_noiter.jsonl
â”‚   â”‚   â”œâ”€â”€ humanevalET_gpt3.5turbo_CoT+M_results.jsonl
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ utils
â”‚       â””â”€â”€ train_set_postprocessing.py
â”œâ”€â”€ code_translation
â”‚   â”œâ”€â”€ Dataset
â”‚   â”œâ”€â”€ evaluate_prompt_java2python.py
â”‚   â”œâ”€â”€ evaluate_prompt_python2java.py
â”‚   â”œâ”€â”€ Java2Python
â”‚   â”‚   â”œâ”€â”€ 1_prompt_mutate_java2python.py
â”‚   â”‚   â”œâ”€â”€ 2_prompt_evaluate_java2python.py
â”‚   â”‚   â”œâ”€â”€ 3_cal_score_and_extract_best_prompt_java2python.py
â”‚   â”‚   â”œâ”€â”€ 4_update_prompt_java2python.py
â”‚   â”‚   â””â”€â”€ res
â”‚   â”‚       â”œâ”€â”€ java2python_results_235_gpt3.5turbo_BPO.jsonl
â”‚   â”‚       â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ Python2Java
â”‚   â”‚   â”œâ”€â”€ 1_prompt_mutate_python2java.py
â”‚   â”‚   â”œâ”€â”€ 2_prompt_evaluate_python2java.py
â”‚   â”‚   â”œâ”€â”€ 3_cal_score_cal_and_extract_best_prompt_python2java.py
â”‚   â”‚   â”œâ”€â”€ 4_update_prompt_python2java.py
â”‚   â”‚   â”œâ”€â”€ res
â”‚   â”‚   â”‚   â”œâ”€â”€ python2java_results_239_gpt3.5turbo_BPO.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ temp
â”‚   â”œâ”€â”€ run_java_testcases_avatar.py
â”‚   â””â”€â”€ run_python_testcases_avatar.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸ™Acknowledgements

Our codebase is based on the following repos. Thanks for open-sourcing!

- [HumanEval](https://github.com/openai/human-eval)
- [MBPP](https://github.com/google-research/google-research/tree/master/mbpp)
- [EvalPlus](https://github.com/evalplus/evalplus)
